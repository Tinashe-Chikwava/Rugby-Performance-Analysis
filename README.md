# üèâ Rugby Performance Analysis

This project is a multi-stage data pipeline designed to retrieve, process, and visualize rugby statistics from the **api-sports.io** API. The final output is a **Quadrant Scatter Plot** that analyzes team performance and consistency.

The pipeline is inspired by the data processing models presented in **Python For Everybody** by Charles Severance.

---

## Key Visualization: Performance vs. Consistency

The core insight of this project is generated by the **Standard Deviation Analysis**, visualized as a scatter plot:

* **X-Axis (Performance):** Average Tries Scored per Game.
* **Y-Axis (Consistency/Volatility):** Standard Deviation of Tries Scored per Game.

Teams in the **bottom-right quadrant** are classified as **Elite and Consistent** (high scoring, low volatility). 
Teams in the **top-left quadrant** are classified as **Poor and Incosistent** (low scoring, high volatility).
This is for ease of reading the actual graph.

---

## Getting Started

To run this project, you need a **Python environment** and install dateTime Util(It works howver without seemingly)
It is neccessary to delete the sqlite files for a fresh install or re-run

### 1. Installation

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/Tinashe-Chikwava/Rugby-Performance-Analysis.git](https://github.com/Tinashe-Chikwava/Rugby-Performance-Analysis.git)
    cd Rugby-Performance-Analysis
    ```

2.  **Install dependencies:**
    ```bash
    pip install requests python-dateutil
    ```

### 2. API Key Setup

The API key should be accessed by you, the original in there in place is a basic one that was put for my readbaility (it was revoked)

1.  Obtain an API key from [v1.rugby.api-sports.io](https://v1.rugby.api-sports.io).
2.  Set the key in the spider.py file and then execute

---

## Data Pipeline Execution

The pipeline must be run in the following order to generate the visualization data.

| Step | File | Purpose & Output | Command |
| :--- | :--- | :--- | :--- |
| **1. Retrieve** | `spider.py` | Connects to the API, retrieves raw JSON data (Leagues/Games/Teams), and stores it in the cache database. | `python spider.py` |
| **2. Model** | `model.py` | Reads raw data, cleans it, and normalizes entities (Teams, Leagues), writing the processed data to the index. | `python model.py` |
| **3. Analyze** | `visualization.py` | Calculates the Average Tries and Standard Deviation metrics for all teams. | `python visualization.py` |

The analysis step creates the final data file: **`Consistency.json`**.

---

## Project Files

| File | Description |
| :--- | :--- |
| `spider.py` | Data acquisition and caching layer. |
| `model.py` | Data cleaning, transformation, and normalization layer. |
| `visualization.py` | Data analysis layer; calculates statistical metrics for visualization. |
| `dump.py` | Utility script for inspecting processed data in the index. |
| `index.html` | Front-end for the D3.js visualization. |
| `visualization.js` | D3.js script that reads the JSON and renders the scatter plot. |

---

## Development Notes:

This project was a learning curve for me as I was learning python for the very first time as I completed the **Python For Everybody** Specialization and this made up the capstone project.

### 1. The API and Key Structure

There initially was a major stuggle with the API key and the structures that came with the JSON's associated these created many errors and took a while to fix initally.

### 2. Database and Data Volume Management

The initial script should be running spider.py and this lead to the usage of many tokens initially, after a while I had enough data that I was happy with in order to continue, I will make sure how to learn how to use sleeps which were suggested and some breaks to not overload the API and run into limits.

### 3. Data Modeling Challenges

Cleaning of the data wasn't very difficult to do otherwise that it was a lot of it and that the data was exremely big, a suggestion made to me was to remove the sql data from the commit to github but i will leave it there for now to show how it works for me. as well as that there is a big api hit as well.

### 4. Advanced Metrics and Statistics

The calculations were the most simple part but led to quite an amount of crashing due to syntax errors made, but ultimately worked up quite well

### 5. Final Visualization

The visualization requiered the most outside assitnace as it needed help from the course and elshewhere to get the visualization.js file as this was quite complicated and beyond my current skill level.