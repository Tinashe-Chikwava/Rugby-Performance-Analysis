## Rugby Performance Analysis: Data Pipeline & Visualization

This project is a multi-stage data pipeline designed to retrieve, process, and visualize rugby statistics from the **api-sports.io** API. The final output is a **Quadrant Scatter Plot** that analyzes team performance and consistency.

The pipeline is inspired by the data processing models presented in **Python For Everybody** by Charles Severance.

---

### Key Visualization: Performance vs. Consistency

The core insight of this project is generated by the **Standard Deviation Analysis**, visualized as a scatter plot:

* **X-Axis (Performance):** Average Tries Scored per Game.
* **Y-Axis (Consistency/Volatility):** Standard Deviation of Tries Scored per Game.

Teams in the **bottom-right quadrant** are classified as **Elite and Consistent** (high scoring, low volatility).
Teams in the **top-left quadrant** are classified as **Poor and Inconsistent** (low scoring, high volatility).
This quadrant classification provides ease of reading the actual graph. 


---

###  Getting Started

To run this project, you need a **Python environment** and the necessary dependencies.
It is necessary to delete the SQLite files for a fresh install or re-run.

#### 1. Installation

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/Tinashe-Chikwava/Rugby-Performance-Analysis.git](https://github.com/Tinashe-Chikwava/Rugby-Performance-Analysis.git)
    cd Rugby-Performance-Analysis
    ```

2.  **Install dependencies:**
    ```bash
    pip install requests python-dateutil
    ```

#### 2. API Key Setup

1.  Obtain an API key from [v1.rugby.api-sports.io](https://v1.rugby.api-sports.io).
2.  Set the key in the `spider.py` file and then execute.

---

### Data Pipeline Execution

The pipeline must be run in the following sequential order to generate the visualization data.

| Step | File | Purpose & Output | Command |
| :--- | :--- | :--- | :--- |
| **1. Retrieve** | `spider.py` | Connects to the API, retrieves raw JSON data (Leagues/Games/Teams), and stores it in the cache database. | `python spider.py` |
| **2. Model** | `model.py` | Reads raw data, cleans it, and normalizes entities (Teams, Leagues), writing the processed data to the index. | `python model.py` |
| **3. Analyze** | `visualization.py` | Calculates the Average Tries and Standard Deviation metrics for all teams. | `python visualization.py` |

The analysis step creates the final data file: **`Consistency.json`**.

---

### Project Files

| File | Description |
| :--- | :--- |
| `spider.py` | Data acquisition and caching layer. |
| `model.py` | Data cleaning, transformation, and normalization layer. |
| `visualization.py` | Data analysis layer; calculates statistical metrics for visualization. |
| `dump.py` | Utility script for inspecting processed data in the index. |
| `index.html` | Front-end for the D3.js visualization. |
| `visualization.js` | D3.js script that reads the JSON and renders the scatter plot. |

---

### Development Notes

This project served as the capstone for the **Python For Everybody** Specialization, marking my first experience learning and applying Python.

* **API Integration:** Initial challenges involved handling the API key and navigating complex JSON structures.
* **Data Management:** Implemented caching to manage API token usage and data volume. Future improvements will include using sleeps/breaks to prevent API rate limits.
* **Data Modeling:** Focused on cleaning and transforming large datasets effectively.
* **Statistical Analysis:** Statistical calculations were executed using Python, leading to the final metrics.
* **Final Visualization:** The D3.js visualization required external resources and assistance due to its complexity.